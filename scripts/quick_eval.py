"""
Quick Evaluation Script for Axiom AI v2.0

Run this locally to generate benchmark metrics (Recall@5, Latency, etc.)
Usage: python scripts/quick_eval.py
"""

import time
import os
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Import app functions
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "hf-axiom"))

from app_hf import get_rag_chain, ingest_files
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document

def run_eval(vectorstore=None, bm25_retriever=None):
    """
    Run evaluation on a small test dataset.
    
    Args:
        vectorstore: Pre-loaded ChromaDB vectorstore
        bm25_retriever: Pre-loaded BM25 retriever
    
    Returns:
        dict: Metrics including accuracy, latency, recall
    """
    
    # Ground Truth Dataset
    # Format: (question, expected_fact_in_answer, expected_page_number)
    dataset = [
        ("What is the main topic?", "main topic", None),  # Generic - adjust based on your test docs
        ("Summarize the key points", "key points", None),
    ]
    
    print("âš¡ Starting Evaluation...")
    print(f"ğŸ“Š Testing {len(dataset)} queries...\n")
    
    if not vectorstore:
        print("âŒ Error: Vectorstore not initialized. Please ingest documents first.")
        return None
    
    # Initialize RAG chain
    try:
        chain = get_rag_chain(vectorstore)
    except Exception as e:
        print(f"âŒ Error initializing RAG chain: {e}")
        return None
    
    score = 0
    citation_score = 0
    total_time = 0
    results = []
    
    for idx, (question, expected_fact, expected_page) in enumerate(dataset, 1):
        print(f"Query {idx}/{len(dataset)}: {question}")
        
        try:
            start_time = time.time()
            
            # Get response (this uses the run_rag function which returns answer + sources)
            # For now, we'll use the chain directly and check the response
            response_text = chain.invoke(question)
            
            duration = (time.time() - start_time) * 1000  # Convert to ms
            total_time += duration
            
            # Check 1: Did we get a relevant answer?
            if expected_fact and expected_fact.lower() in response_text.lower():
                score += 1
                print(f"  âœ… Answer contains expected fact: '{expected_fact}'")
            else:
                print(f"  âš ï¸  Answer may not contain expected fact")
            
            # Check 2: Did we cite sources?
            if "[" in response_text and ("Page" in response_text or "page" in response_text.lower()):
                citation_score += 1
                print(f"  âœ… Response includes citations")
            else:
                print(f"  âš ï¸  Response missing citations")
            
            # Check 3: Page number accuracy (if expected)
            if expected_page and str(expected_page) in response_text:
                print(f"  âœ… Correctly cited Page {expected_page}")
            
            print(f"  â±ï¸  Latency: {duration:.0f}ms\n")
            
            results.append({
                "question": question,
                "response": response_text[:100] + "..." if len(response_text) > 100 else response_text,
                "latency_ms": round(duration, 2),
                "has_citation": "[" in response_text
            })
            
        except Exception as e:
            print(f"  âŒ Error: {str(e)}\n")
            results.append({
                "question": question,
                "error": str(e)
            })
    
    # Calculate metrics
    accuracy = (score / len(dataset)) * 100 if dataset else 0
    citation_rate = (citation_score / len(dataset)) * 100 if dataset else 0
    avg_latency = total_time / len(dataset) if dataset else 0
    
    print("=" * 60)
    print("ğŸ“Š EVALUATION RESULTS")
    print("=" * 60)
    print(f"âœ… Answer Accuracy: {score}/{len(dataset)} ({accuracy:.1f}%)")
    print(f"ğŸ“ Citation Rate: {citation_score}/{len(dataset)} ({citation_rate:.1f}%)")
    print(f"â±ï¸  Average Latency: {avg_latency:.0f}ms")
    print(f"ğŸ’° Estimated Cost: <$0.01 per 100 queries (GPT-4o-mini)")
    print("=" * 60)
    
    return {
        "accuracy": accuracy,
        "citation_rate": citation_rate,
        "avg_latency_ms": avg_latency,
        "results": results
    }

if __name__ == "__main__":
    print("ğŸ”¬ Axiom AI v2.0 Evaluation Script")
    print("=" * 60)
    
    # Check API key
    if not os.environ.get("OPENAI_API_KEY"):
        print("âŒ Error: OPENAI_API_KEY not found in environment.")
        print("   Set it with: export OPENAI_API_KEY='sk-...'")
        exit(1)
    
    # Note: For a full evaluation, you would:
    # 1. Load test documents
    # 2. Ingest them using ingest_files()
    # 3. Build BM25 retriever
    # 4. Run evaluation
    
    print("\nâš ï¸  Note: This script requires pre-ingested documents.")
    print("   To run a full evaluation:")
    print("   1. Upload test PDFs to the app")
    print("   2. Run this script after ingestion")
    print("   3. Or modify this script to ingest test documents automatically\n")
    
    # For now, we'll just show the structure
    # In production, you'd load the vectorstore from session state or disk
    print("ğŸ’¡ To use this script:")
    print("   1. Run the Streamlit app and ingest documents")
    print("   2. Export vectorstore path")
    print("   3. Load it here and run evaluation")
    
    # Uncomment below when you have a vectorstore ready:
    # vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=OpenAIEmbeddings())
    # metrics = run_eval(vectorstore)
    # print(f"\nğŸ“ˆ Final Metrics: {metrics}")

