# Axiom AI - System Design (Demo Scale)

This document provides a simplified, demo-scale view of the Axiom AI RAG pipeline.

## High-Level RAG Pipeline (Simplified)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AXIOM AI RAG SYSTEM                         â”‚
â”‚                         (Demo Scale)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 1: INGESTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Documents (PDF/TXT)
           â†“
    [Text Extraction]
           â†“
    [Chunking: 800 chars]
           â†“
    [Embedding: MiniLM-L6-v2]
           â†“
    [Store in ChromaDB]


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 2: QUERY                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    User Question (EN/HI)
           â†“
    [Embed Query]
           â†“
    [Vector Search â†’ Top 5 chunks]
           â†“
    [LLM (GPT-4o) + Context]
           â†“
    Answer + Citations


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PHASE 3: EVALUATION                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Test Set (33 queries)
           â†“
    [Run through Query Engine]
           â†“
    [Measure: Recall@k, MRR, Latency]
           â†“
    Baseline Metrics (100% accuracy)
```

---

## Component Responsibilities (Demo Scale)

| Component | What It Does | Technology |
|-----------|--------------|------------|
| **Document Loader** | Reads PDF/TXT files | PyPDF2, built-in |
| **Chunker** | Splits text into 800-char pieces | Custom logic |
| **Embedder** | Converts text â†’ 384-d vectors | sentence-transformers |
| **Vector DB** | Stores & searches embeddings | ChromaDB (local) |
| **Query Engine** | Orchestrates retrieval | Custom Python |
| **LLM** | Generates answers from context | OpenAI GPT-4o |
| **Evaluator** | Measures retrieval quality | pytrec-eval |

---

## Data Flow Example

### Ingestion Example
```
Input: "axiom_doc.pdf" (5 pages, English + Hindi)
       â†“
Step 1: Extract text â†’ "Axiom AI is a RAG system..."
       â†“
Step 2: Chunk â†’ ["Axiom AI is a RAG...", "The system uses...", ...]
       â†“
Step 3: Embed â†’ [[0.23, -0.45, ...], [0.12, 0.87, ...], ...]
       â†“
Output: 12 chunks stored in ChromaDB
```

### Query Example
```
Input: "What is Axiom AI?" (English)
       â†“
Step 1: Embed query â†’ [0.21, -0.43, 0.67, ...]
       â†“
Step 2: Search ChromaDB â†’ Top 5 similar chunks
       â†“
Step 3: Build prompt:
        "Based on these chunks:
         1. 'Axiom AI is a RAG system...'
         2. 'The system uses embeddings...'
         Answer: What is Axiom AI?"
       â†“
Step 4: GPT-4o generates answer
       â†“
Output: "Axiom AI is a multilingual RAG system that..." + [citations]
```

### Evaluation Example
```
Input: Test set with 3 queries
       â†“
Query 1: "What is AxiomAI?" â†’ Retrieved: eval_doc_1.txt âœ“
Query 2: "Core components?" â†’ Retrieved: eval_doc_1.txt âœ“
Query 3: "Project mascot?" â†’ Retrieved: eval_doc_1.txt âœ“
       â†“
Metrics:
  - Recall@1: 3/3 = 100%
  - Recall@5: 3/3 = 100%
  - MRR: (1/1 + 1/1 + 1/1)/3 = 1.0
  - Avg Latency: 117ms
       â†“
Output: baseline_en.json
```

---

## Scale Characteristics (Demo vs Production)

| Aspect | Demo Scale (Current) | Production Scale (Future) |
|--------|---------------------|---------------------------|
| **Documents** | ~5-10 documents | 10,000+ documents |
| **Vector DB** | ChromaDB (local file) | ChromaDB cluster / Pinecone |
| **Concurrency** | Single-threaded | Async + thread pool |
| **Caching** | None | Redis for embeddings |
| **Monitoring** | Basic metrics | Grafana + Prometheus |
| **Deployment** | Local Python script | Docker + Kubernetes |
| **API** | None (CLI only) | REST API + rate limiting |

---

## Design Decisions (Demo Scale Rationale)

### âœ… What We Chose (and Why)

1. **Local ChromaDB** (not cloud vector DB)
   - âœ… No API costs
   - âœ… Works offline
   - âœ… Fast for demo corpus (<100 docs)
   - âŒ Doesn't scale to millions of docs

2. **sentence-transformers** (not OpenAI embeddings)
   - âœ… Free, runs locally
   - âœ… Multilingual support
   - âœ… 384-d (smaller than OpenAI's 1536-d)
   - âŒ Slightly lower quality than OpenAI

3. **OpenAI GPT-4o** (not local LLM)
   - âœ… Best-in-class generation quality
   - âœ… Reliable API
   - âŒ Costs ~$0.01 per query
   - âŒ Requires internet

4. **No API layer** (direct Python script)
   - âœ… Simpler to demo
   - âœ… Easier to debug
   - âŒ Can't serve multiple users
   - âŒ Not production-ready

### ğŸ¯ When to Upgrade

| Trigger | Upgrade Path |
|---------|--------------|
| **>1000 documents** | Move to Pinecone or Weaviate |
| **>10 QPS** | Add async + FastAPI endpoint |
| **Cost concerns** | Switch to local LLM (Llama 3.1) |
| **Multi-user** | Add authentication + session management |

---

## Success Metrics (Demo Scale)

### âœ… What "Good" Looks Like

| Metric | Target | Actual | Status |
|--------|--------|--------|--------|
| **Recall@10** | >80% | 100% | âœ… Exceeds |
| **MRR** | >0.7 | 1.0 | âœ… Exceeds |
| **Latency** | <500ms | 117ms | âœ… Exceeds |
| **Multilingual** | 2+ languages | 2 (EN, HI) | âœ… Met |
| **Test Coverage** | >20 queries | 33 queries | âœ… Exceeds |

---

## Quick Start (2 Commands)

```bash
# 1. Ingest a document
python scripts/ingest.py path/to/document.pdf

# 2. Query the system
python -m axiom.query --query "Your question here"
```

---

## What Makes This "Demo Scale"?

**Optimized for:**
- âœ… Rapid iteration
- âœ… Easy debugging
- âœ… Clear demonstrations
- âœ… Validated correctness

**NOT optimized for:**
- âŒ High throughput (>100 QPS)
- âŒ Large corpus (>10K docs)
- âŒ Multi-tenancy
- âŒ Edge cases / adversarial inputs

**Perfect for:**
- Technical interviews
- Proof-of-concept demos
- Learning RAG fundamentals
- Architecture discussions

---

## Interview Talking Points

**"This is a demo-scale RAG system optimized for clarity and validation rather than production scale. The architecture is modular, so scaling components like the vector store or adding an API layer is straightforward. I validated correctness with comprehensive evaluation harnesses showing 100% retrieval accuracy across English and Hindi queries."**

---

*Last updated: 2025-10-28*
*Status: Demo-ready, evaluation-validated, multilingual-capable*

